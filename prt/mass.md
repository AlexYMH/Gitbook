---
description: >-
  MASS全称：“Masked Sequence to Sequence Pre-training”，Paper：“MASS: Masked Sequence
  to Sequence Pre-training for Language Generation”。
---

# MASS

在序列到序列的自然语言生成任务中，目前主流预训练模型并没有取得显著效果。为此，微软亚洲研究院的研究员在ICML 2019上提出了**一个全新的通用预训练方法MASS，在序列到序列的自然语言生成任务中全面超越BERT和GPT。**在微软参加的WMT19机器翻译比赛中，MASS帮助中-英、英-立陶宛两个语言对取得了第一名的成绩。

## MASS结构

MASS是专门针对序列到序列的自然语言生成任务的预训练方法。



