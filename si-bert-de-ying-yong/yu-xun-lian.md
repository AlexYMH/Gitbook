---
description: 基础。
---

# 预训练

词嵌入的学习其实是NLP中的预训练技术，而NLP中的预训练是从CV中的预训练借鉴过来的。

### **预训练的原因**

* 训练数据小，不足以训练复杂的网络；
* 加快训练速度；
* 参数初始化，先找到好的初始点，有利于优化。

### **预训练好的模型的使用方法**

* 当新数据集较小，并且与原始数据集相似时，我们可以**直接使用预训练好的模型参数**，不进行fine-tune，只需要在最后添加一层与任务相关的网络层；
* 当新数据集较大，并且与原始数据集相似时，我们可以**在预训练好的整个模型参数上进行fine-tune；**
* 当新数据集较小，但与原始数据集不同时，我们可以**使用预训练好的模型浅层的参数**，然后再在后面添加一个与任务相关的网络层；
* 当新数据集较大，但与原始数据不同时，我们可以**在预训练好的整个模型参数上进行fine-tune。**

### **预训练模型参数的特点**

* **浅层特征的可复用性：**底层特征一般对于任意数据集都是通用的，例如图形的边、角等特征；
* **高层特征的任务相关性：**而高层特征一般都是与任务相关的。

### **预训练的两类方法**

* **Feature-based Pre-Training\(基于特征融合的预训练方法\)：**例如ELMo
* **基于Fine-tuning的模式**：如ULM-Fit、GPT1.0

我们如果把ELMO这种预训练方法和图像领域的预训练方法对比，发现两者模式看上去还是有很大差异的。除了以ELMO为代表的这种**基于特征融合的预训练方法**外，NLP里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为“**基于Fine-tuning的模式**”，而GPT就是这一模式的典型开创者。

### NLP预训练的本质

**本质上预训练是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，集成尽量通用的语言学知识自然会加强模型的泛化能力。**如何引入先验的语言学知识其实一直是NLP尤其是深度学习场景下的NLP的主要目标之一，不过一直没有太好的解决办法，而ELMO/GPT/Bert的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在。



